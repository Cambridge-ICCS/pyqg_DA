{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "import DA_core as DA\n",
    "from glob import glob\n",
    "import torch.utils.data as Data\n",
    "from torch import optim\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import ML_core as ML\n",
    "from numpy.random import default_rng\n",
    "import os\n",
    "\n",
    "rng = default_rng()\n",
    "DA.read_data_dir='./data'\n",
    "DA.save_data_dir='./data'\n",
    "\n",
    "data_dir='./data'\n",
    "# data_dir='/net2/fnl/PyQG/data'\n",
    "B_ens_kws={'cmap':'bwr','levels':np.linspace(-2.5E-11,2.5E-11,26),'extend':'both'}\n",
    "B_ens_kws1={'cmap':'bwr','levels':np.linspace(-0.25E-11,0.25E-11,26),'extend':'both'}\n",
    "q_kws={'cmap':'bwr','levels':np.linspace(-3.4E-5,3.4E-5,18),'extend':'both'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select experiment for training data\n",
    "DA_paras={'nens':80,\n",
    "          'DA_method':'EnKF',\n",
    "          'Nx_DA':32,\n",
    "          'Nx_truth':128,\n",
    "          'obs_freq':10,\n",
    "          'obs_err':[1,-5,5,-7],\n",
    "          'DA_freq':10,\n",
    "          'save_B':False,\n",
    "          'nobs':[50,50],\n",
    "          'R_W':100,\n",
    "          'inflate':[1,0.5]}\n",
    "DA_exp=DA.DA_exp(**DA_paras)\n",
    "print(DA_exp.file_name())\n",
    "# obs_ds=DA_exp.read_obs()\n",
    "in_ch=[0,1]\n",
    "out_ch=[0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ensemble-mean q and ensemble covariance B for the selected DA experiment\n",
    "mean_ds=DA_exp.read_mean().load()\n",
    "print(mean_ds.q.shape)\n",
    "# if DA_exp.nens>1:  \n",
    "#     std_ds=DA_exp.read_std()\n",
    "\n",
    "# Read the full covariance matrix for all time steps and model gridpoints\n",
    "B_ens_ds=xr.open_dataset('{}/training/{}/B_sample.nc'.format(data_dir,DA_exp.file_name()))\n",
    "print(B_ens_ds.B_ens.shape)\n",
    "\n",
    "ml_std_ds=xr.open_dataset('./data/std_{0}.nc'.format(DA_exp.file_name()))\n",
    "print(ml_std_ds)\n",
    "\n",
    "B_R=int((len(B_ens_ds.x_d)-1)/2)\n",
    "B_size=16\n",
    "B_start=0\n",
    "\n",
    "DA_days=slice(9,365,DA_exp.DA_freq)\n",
    "DA_it=slice(int((DA_days.start-DA_exp.DA_freq+1)/DA_exp.DA_freq),int((DA_days.stop-DA_exp.DA_freq+1)/DA_exp.DA_freq)+1)\n",
    "print(DA_days,DA_it)\n",
    "i_x=np.arange(0,DA_exp.Nx_DA)\n",
    "i_y=np.arange(0,DA_exp.Nx_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_shape=B_ens_ds.B_ens.isel(time=DA_it,y=i_y,x=i_x).shape\n",
    "print(B_shape)\n",
    "print(mean_ds.q.isel(time=DA_days).shape)\n",
    "B_nt=B_shape[0]\n",
    "B_ny=B_shape[2]\n",
    "B_nx=B_shape[3]\n",
    "B_total=B_nt*B_ny*B_nx\n",
    "print(B_total)\n",
    "n_train=int(B_total*0.8)\n",
    "# rngs=rng.permutation(B_total)\n",
    "rngs=np.arange(B_total)\n",
    "partition={'train':rngs[0:n_train],'valid':rngs[n_train:]}\n",
    "        \n",
    "train_ds=ML.Dataset(mean_ds.q.isel(time=DA_days),DA_exp.Nx_DA,\n",
    "                    B_ens_ds.B_ens.isel(time=DA_it,y=i_y,x=i_x),i_y,i_x,\n",
    "                    partition['train'],ml_std_ds.q_std.data,ml_std_ds.B_std.data,in_ch,out_ch,B_size=B_size,B_start=B_start)\n",
    "valid_ds=ML.Dataset(mean_ds.q.isel(time=DA_days),DA_exp.Nx_DA,\n",
    "                    B_ens_ds.B_ens.isel(time=DA_it,y=i_y,x=i_x),i_y,i_x,\n",
    "                    partition['valid'],ml_std_ds.q_std.data,ml_std_ds.B_std.data,in_ch,out_ch,B_size=B_size,B_start=B_start)\n",
    "\n",
    "params = {'batch_size':16,'num_workers':1,'shuffle':True}\n",
    "training_generator = torch.utils.data.DataLoader(train_ds, **params)\n",
    "validation_generator = torch.utils.data.DataLoader(valid_ds, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate level-wise standard deviations for normalization\n",
    "B_std=np.empty((2,2))\n",
    "B_std[0,0]=np.std(B_ens_ds.B_ens.isel(time=DA_it,lev=0,lev_d=0))\n",
    "B_std[0,1]=np.std(B_ens_ds.B_ens.isel(time=DA_it,lev=0,lev_d=1))\n",
    "B_std[1,0]=B_std[0,1]\n",
    "B_std[1,1]=np.std(B_ens_ds.B_ens.isel(time=DA_it,lev=1,lev_d=1))\n",
    "print(B_std)\n",
    "\n",
    "q_std=np.zeros((2,1))\n",
    "q_std[0]=np.std(mean_ds.q.isel(time=DA_days,lev=0))\n",
    "q_std[1]=np.std(mean_ds.q.isel(time=DA_days,lev=1))\n",
    "print(q_std)\n",
    "\n",
    "B_da=xr.DataArray(B_std,coords=[mean_ds.lev,mean_ds.lev])\n",
    "q_da=xr.DataArray(q_std.squeeze(),coords=[mean_ds.lev])\n",
    "std_ds=xr.Dataset({'B_std':B_da,'q_std':q_da})\n",
    "std_ds.to_netcdf('./ML/{0}/std_{0}.nc'.format(DA_exp.file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model=ML.Unet(in_ch=len(in_ch),out_ch=len(out_ch))\n",
    "model=model.to(device)\n",
    "print(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "summary(model, input_size=(len(in_ch),train_ds.B_size,train_ds.B_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() # MSE loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.double()\n",
    "n_epochs = 30 #Number of epocs\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "start_epoch=0\n",
    "if start_epoch>0:\n",
    "    model_file='./ML/unet_epoch{}_in{}_out{}_B{}_{}.pt'.format(start_epoch,''.join(map(str,in_ch)),''.join(map(str,out_ch)),B_size,DA_exp.file_name())\n",
    "    print(model_file)\n",
    "    model.load_state_dict(torch.load(model_file,map_location=torch.device('cpu')))\n",
    "# time0 = time()  \n",
    "for epoch in range(start_epoch+1, n_epochs + 1):\n",
    "    train_loss.append(ML.train_model(model,criterion,training_generator,optimizer,device))\n",
    "    # validation_loss.append(ML.test_model(model,criterion,validation_generator,optimizer,device))\n",
    "    torch.save(model.state_dict(), './ML/{}/unet_epoch{}_in{}_out{}_B{}_{}.pt'.\\\n",
    "        format(DA_exp.file_name(),epoch,''.join(map(str,in_ch)),''.join(map(str,out_ch)),B_size,DA_exp.file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,'b', label='training loss');\n",
    "plt.plot(validation_loss,'r', label='validation loss');\n",
    "\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyqg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6973bb065886c44b729be95666fc7914cdffcda5c95a8d198f96362135dd92cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
